transformers==4.36.2
tokenizers==0.15.0
torch>=2.0.0
accelerate>=0.20.3
peft>=0.6.0
evaluate>=0.4.0
datasets>=2.14.5
scipy>=1.11.3
scikit-learn>=1.3.2
sentencepiece>=0.1.99
protobuf>=4.23.4
huggingface_hub>=0.17.3
bitsandbytes>=0.41.1
nltk>=3.8.1

# Adapter management and cloud storage
boto3>=1.28.0
google-cloud-storage>=2.10.0
gradio>=4.0.0

# Optional: For optimized inference
flash-attn>=2.3.4
triton>=2.1.0 